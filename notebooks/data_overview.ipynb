{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd463ac7",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Cybersecurity Dataset Overview & Analysis\n",
    "\n",
    "## üìä Comprehensive Data Analysis for Data Analyst Internship\n",
    "\n",
    "This notebook provides a thorough overview and analysis of the CloudWatch Traffic Web Attack dataset:\n",
    "- **Dataset Profiling** - Shape, structure, and data quality assessment\n",
    "- **Statistical Analysis** - Descriptive statistics and distributions\n",
    "- **Data Visualization** - Charts and graphs for better understanding\n",
    "- **Data Quality Assessment** - Missing values, duplicates, and anomalies\n",
    "- **Feature Analysis** - Individual column analysis and insights\n",
    "\n",
    "**Objective:** Understanding the cybersecurity dataset to identify threats and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267199e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset Shape: (282, 16)\n",
      "\n",
      "‚úÖ Column Names:\n",
      "['bytes_in', 'bytes_out', 'creation_time', 'end_time', 'src_ip', 'src_ip_country_code', 'protocol', 'response.code', 'dst_port', 'dst_ip', 'rule_names', 'observation_name', 'source.meta', 'source.name', 'time', 'detection_types']\n",
      "\n",
      "‚úÖ First 5 Rows:\n",
      "   bytes_in  bytes_out         creation_time              end_time  \\\n",
      "0      5602      12990  2024-04-25T23:00:00Z  2024-04-25T23:10:00Z   \n",
      "1     30912      18186  2024-04-25T23:00:00Z  2024-04-25T23:10:00Z   \n",
      "2     28506      13468  2024-04-25T23:00:00Z  2024-04-25T23:10:00Z   \n",
      "3     30546      14278  2024-04-25T23:00:00Z  2024-04-25T23:10:00Z   \n",
      "4      6526      13892  2024-04-25T23:00:00Z  2024-04-25T23:10:00Z   \n",
      "\n",
      "            src_ip src_ip_country_code protocol  response.code  dst_port  \\\n",
      "0   147.161.161.82                  AE    HTTPS            200       443   \n",
      "1     165.225.33.6                  US    HTTPS            200       443   \n",
      "2  165.225.212.255                  CA    HTTPS            200       443   \n",
      "3   136.226.64.114                  US    HTTPS            200       443   \n",
      "4   165.225.240.79                  NL    HTTPS            200       443   \n",
      "\n",
      "         dst_ip              rule_names                      observation_name  \\\n",
      "0  10.138.69.97  Suspicious Web Traffic  Adversary Infrastructure Interaction   \n",
      "1  10.138.69.97  Suspicious Web Traffic  Adversary Infrastructure Interaction   \n",
      "2  10.138.69.97  Suspicious Web Traffic  Adversary Infrastructure Interaction   \n",
      "3  10.138.69.97  Suspicious Web Traffic  Adversary Infrastructure Interaction   \n",
      "4  10.138.69.97  Suspicious Web Traffic  Adversary Infrastructure Interaction   \n",
      "\n",
      "    source.meta     source.name                  time detection_types  \n",
      "0  AWS_VPC_Flow  prod_webserver  2024-04-25T23:00:00Z        waf_rule  \n",
      "1  AWS_VPC_Flow  prod_webserver  2024-04-25T23:00:00Z        waf_rule  \n",
      "2  AWS_VPC_Flow  prod_webserver  2024-04-25T23:00:00Z        waf_rule  \n",
      "3  AWS_VPC_Flow  prod_webserver  2024-04-25T23:00:00Z        waf_rule  \n",
      "4  AWS_VPC_Flow  prod_webserver  2024-04-25T23:00:00Z        waf_rule  \n",
      "\n",
      "‚úÖ Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 282 entries, 0 to 281\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   bytes_in             282 non-null    int64 \n",
      " 1   bytes_out            282 non-null    int64 \n",
      " 2   creation_time        282 non-null    object\n",
      " 3   end_time             282 non-null    object\n",
      " 4   src_ip               282 non-null    object\n",
      " 5   src_ip_country_code  282 non-null    object\n",
      " 6   protocol             282 non-null    object\n",
      " 7   response.code        282 non-null    int64 \n",
      " 8   dst_port             282 non-null    int64 \n",
      " 9   dst_ip               282 non-null    object\n",
      " 10  rule_names           282 non-null    object\n",
      " 11  observation_name     282 non-null    object\n",
      " 12  source.meta          282 non-null    object\n",
      " 13  source.name          282 non-null    object\n",
      " 14  time                 282 non-null    object\n",
      " 15  detection_types      282 non-null    object\n",
      "dtypes: int64(4), object(12)\n",
      "memory usage: 35.4+ KB\n",
      "None\n",
      "\n",
      "‚úÖ Missing Values:\n",
      "bytes_in               0\n",
      "bytes_out              0\n",
      "creation_time          0\n",
      "end_time               0\n",
      "src_ip                 0\n",
      "src_ip_country_code    0\n",
      "protocol               0\n",
      "response.code          0\n",
      "dst_port               0\n",
      "dst_ip                 0\n",
      "rule_names             0\n",
      "observation_name       0\n",
      "source.meta            0\n",
      "source.name            0\n",
      "time                   0\n",
      "detection_types        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('üöÄ Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb114c",
   "metadata": {},
   "source": [
    "## üìÇ 1. Dataset Loading & Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print('üìÇ Loading CloudWatch Traffic Web Attack Dataset...')\n",
    "df = pd.read_csv(\"../data/CloudWatch_Traffic_Web_Attack.csv\")\n",
    "\n",
    "# Display dataset shape\n",
    "print(f'‚úÖ Dataset Shape: {df.shape}')\n",
    "print(f'üìä Rows: {df.shape[0]:,} | Columns: {df.shape[1]}')\n",
    "print(f'üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
    "\n",
    "# Show column names\n",
    "print(f'\\nüìù Column Names ({len(df.columns)} total):')\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f'  {i:2d}. {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 5 rows\n",
    "print('üìã First 5 Rows:')\n",
    "display(df.head())\n",
    "\n",
    "# View last 5 rows\n",
    "print('\\nüìã Last 5 Rows:')\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c32bc9",
   "metadata": {},
   "source": [
    "## üîç 2. Data Types & Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32573e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and nulls\n",
    "print('üîç DATASET INFO:')\n",
    "print('='*40)\n",
    "df.info()\n",
    "\n",
    "# Data types summary\n",
    "print('\\nüìä Data Types Summary:')\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f'  {dtype}: {count} columns')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f'\\nüìà Numeric Columns ({len(numeric_cols)}): {numeric_cols}')\n",
    "print(f'üè∑Ô∏è Categorical Columns ({len(categorical_cols)}): {categorical_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('üîç MISSING VALUES ANALYSIS:')\n",
    "print('='*35)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing Count': missing_data.values,\n",
    "    'Missing %': missing_percent.values\n",
    "})\n",
    "\n",
    "# Only show columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print('Columns with missing values:')\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print('‚úÖ No missing values found!')\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f'\\nüîÑ Duplicate Rows: {duplicate_count:,} ({duplicate_count/len(df)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68d099",
   "metadata": {},
   "source": [
    "## üìà 3. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98326197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numeric columns\n",
    "print('üìà DESCRIPTIVE STATISTICS:')\n",
    "print('='*35)\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    stats = df[numeric_cols].describe()\n",
    "    display(stats)\n",
    "    \n",
    "    # Additional statistics\n",
    "    print('\\nüìä Additional Statistics:')\n",
    "    for col in numeric_cols:\n",
    "        print(f'\\n{col}:')\n",
    "        print(f'  ‚Ä¢ Variance: {df[col].var():.2f}')\n",
    "        print(f'  ‚Ä¢ Skewness: {df[col].skew():.2f}')\n",
    "        print(f'  ‚Ä¢ Kurtosis: {df[col].kurtosis():.2f}')\n",
    "        print(f'  ‚Ä¢ Range: {df[col].max() - df[col].min():.2f}')\n",
    "        print(f'  ‚Ä¢ IQR: {df[col].quantile(0.75) - df[col].quantile(0.25):.2f}')\n",
    "else:\n",
    "    print('No numeric columns found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data analysis\n",
    "print('üè∑Ô∏è CATEGORICAL DATA ANALYSIS:')\n",
    "print('='*35)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f'\\nüìä {col}:')\n",
    "    print(f'  ‚Ä¢ Unique values: {df[col].nunique()}')\n",
    "    print(f'  ‚Ä¢ Most frequent: {df[col].mode().iloc[0] if not df[col].mode().empty else \"N/A\"}')\n",
    "    \n",
    "    # Show top 10 values\n",
    "    top_values = df[col].value_counts().head(10)\n",
    "    print(f'  ‚Ä¢ Top values:')\n",
    "    for val, count in top_values.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f'    - {val}: {count:,} ({percentage:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54097a",
   "metadata": {},
   "source": [
    "## üìä 4. Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for numeric columns\n",
    "if len(numeric_cols) > 0:\n",
    "    # Distribution plots\n",
    "    n_cols = min(4, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    fig.suptitle('üìä Distribution of Numeric Features', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        if i < len(axes):\n",
    "            axes[i].hist(df[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_title(f'{col} Distribution')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "if len(numeric_cols) > 0:\n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    fig.suptitle('üì¶ Box Plots for Outlier Detection', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        if i < len(axes):\n",
    "            data = df[col].dropna()\n",
    "            axes[i].boxplot(data)\n",
    "            axes[i].set_title(f'{col} Box Plot')\n",
    "            axes[i].set_ylabel(col)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data visualizations\n",
    "for col in categorical_cols[:3]:  # Limit to first 3 categorical columns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Get top 15 values to avoid overcrowding\n",
    "    top_values = df[col].value_counts().head(15)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    top_values.plot(kind='bar')\n",
    "    plt.title(f'{col} - Top 15 Values (Bar Chart)')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_values.plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title(f'{col} - Distribution (Pie Chart)')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437de02",
   "metadata": {},
   "source": [
    "## üîó 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for numeric columns\n",
    "if len(numeric_cols) > 1:\n",
    "    print('üîó CORRELATION ANALYSIS:')\n",
    "    print('='*30)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f', cbar_kws={'shrink': .8})\n",
    "    plt.title('üîó Correlation Matrix Heatmap', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print('\\nüî¥ Highly Correlated Features (|correlation| > 0.7):')\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f'  ‚Ä¢ {feat1} ‚Üî {feat2}: {corr:.3f}')\n",
    "    else:\n",
    "        print('\\n‚úÖ No highly correlated features found')\n",
    "else:\n",
    "    print('Need at least 2 numeric columns for correlation analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a58f0",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è 6. Data Quality Issues & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55103aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print('‚ö†Ô∏è DATA QUALITY ASSESSMENT:')\n",
    "print('='*35)\n",
    "\n",
    "issues = []\n",
    "recommendations = []\n",
    "\n",
    "# Check for missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    issues.append('Missing values detected')\n",
    "    recommendations.append('Handle missing values through imputation or removal')\n",
    "\n",
    "# Check for duplicates\n",
    "if df.duplicated().sum() > 0:\n",
    "    issues.append(f'{df.duplicated().sum()} duplicate rows found')\n",
    "    recommendations.append('Remove duplicate rows to avoid bias')\n",
    "\n",
    "# Check for potential outliers in numeric columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)][col]\n",
    "    if len(outliers) > 0:\n",
    "        issues.append(f'{len(outliers)} potential outliers in {col}')\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    recommendations.append('Investigate and handle outliers appropriately')\n",
    "\n",
    "# Check for high cardinality categorical columns\n",
    "for col in categorical_cols:\n",
    "    unique_ratio = df[col].nunique() / len(df)\n",
    "    if unique_ratio > 0.9:\n",
    "        issues.append(f'High cardinality in {col} ({df[col].nunique()} unique values)')\n",
    "        recommendations.append(f'Consider feature engineering for {col}')\n",
    "\n",
    "# Display issues and recommendations\n",
    "if issues:\n",
    "    print('üî¥ Issues Found:')\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f'  {i}. {issue}')\n",
    "    \n",
    "    print('\\nüí° Recommendations:')\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f'  {i}. {rec}')\n",
    "else:\n",
    "    print('‚úÖ No major data quality issues found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928530e",
   "metadata": {},
   "source": [
    "## üìã 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print('üìã DATASET SUMMARY REPORT')\n",
    "print('='*40)\n",
    "\n",
    "summary_stats = {\n",
    "    'Total Records': f\"{len(df):,}\",\n",
    "    'Total Features': len(df.columns),\n",
    "    'Numeric Features': len(numeric_cols),\n",
    "    'Categorical Features': len(categorical_cols),\n",
    "    'Memory Usage': f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\",\n",
    "    'Missing Values': df.isnull().sum().sum(),\n",
    "    'Duplicate Rows': df.duplicated().sum(),\n",
    "    'Data Quality Score': f\"{max(0, 100 - len(issues)*10)}/100\"\n",
    "}\n",
    "\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# Feature importance insights\n",
    "if len(numeric_cols) > 0:\n",
    "    print('\\nüéØ Key Insights:')\n",
    "    \n",
    "    # Find columns with highest variance (potential importance)\n",
    "    variances = df[numeric_cols].var().sort_values(ascending=False)\n",
    "    print(f'‚Ä¢ Highest variance feature: {variances.index[0]} ({variances.iloc[0]:.2f})')\n",
    "    \n",
    "    # Find most common categorical values\n",
    "    for col in categorical_cols[:2]:  # Show first 2 categorical columns\n",
    "        mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else 'N/A'\n",
    "        mode_count = (df[col] == mode_val).sum()\n",
    "        print(f'‚Ä¢ Most common {col}: {mode_val} ({mode_count:,} occurrences)')\n",
    "\n",
    "print('\\n‚úÖ Dataset overview analysis completed!')\n",
    "print('üöÄ Ready for advanced analysis and machine learning!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
