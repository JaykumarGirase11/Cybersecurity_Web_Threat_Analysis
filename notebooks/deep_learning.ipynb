{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d76041",
   "metadata": {},
   "source": [
    "# ğŸ›¡ï¸ Cybersecurity Web Threat Analysis - Deep Learning & Advanced Analytics\n",
    "\n",
    "## ğŸ“Š Comprehensive Dataset Analysis for Data Analyst Internship\n",
    "\n",
    "This notebook provides an in-depth analysis of the CloudWatch Traffic Web Attack dataset using:\n",
    "- **Statistical Analysis** - Descriptive statistics and data profiling\n",
    "- **Data Visualization** - Advanced charts and interactive plots\n",
    "- **Deep Learning Models** - Neural networks for threat detection\n",
    "- **Feature Engineering** - Advanced feature creation and selection\n",
    "- **Anomaly Detection** - Multiple ML approaches for threat identification\n",
    "\n",
    "**Dataset Focus:** Network traffic analysis for cybersecurity threat detection\n",
    "**Tools Used:** Python, Pandas, Scikit-learn, TensorFlow, Plotly, Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92954085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "# Deep Learning Libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    print('âœ… TensorFlow imported successfully')\n",
    "except ImportError:\n",
    "    print('âš ï¸ TensorFlow not available, using sklearn models only')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('ğŸš€ All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06960df7",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 1. Dataset Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9793b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ Load the cybersecurity dataset\n",
    "print('ğŸ“‚ Loading CloudWatch Traffic Web Attack Dataset...')\n",
    "df = pd.read_csv('../data/CloudWatch_Traffic_Web_Attack.csv')\n",
    "\n",
    "print(f'âœ… Dataset loaded successfully!')\n",
    "print(f'ğŸ“Š Dataset Shape: {df.shape}')\n",
    "print(f'ğŸ’¾ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
    "\n",
    "# Display basic information\n",
    "print('\\n' + '='*60)\n",
    "print('ğŸ“‹ DATASET OVERVIEW')\n",
    "print('='*60)\n",
    "print(f'Total Records: {len(df):,}')\n",
    "print(f'Total Features: {len(df.columns)}')\n",
    "print(f'Duplicate Records: {df.duplicated().sum()}')\n",
    "print(f'Missing Values: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Detailed Column Analysis\n",
    "print('ğŸ” COLUMN ANALYSIS')\n",
    "print('='*50)\n",
    "\n",
    "# Data types analysis\n",
    "print('ğŸ“Š Data Types:')\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print('\\nğŸ“ Column Details:')\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    dtype = df[col].dtype\n",
    "    null_count = df[col].isnull().sum()\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f'{i:2d}. {col:<25} | Type: {str(dtype):<12} | Nulls: {null_count:>6} | Unique: {unique_count:>8}')\n",
    "\n",
    "# Display sample data\n",
    "print('\\nğŸ“‹ First 5 Rows:')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980f2ee",
   "metadata": {},
   "source": [
    "## ğŸ§¹ 2. Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¹ Data Cleaning Process\n",
    "print('ğŸ§¹ Starting Data Cleaning Process...')\n",
    "print('='*40)\n",
    "\n",
    "# 1. Remove duplicates\n",
    "initial_rows = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f'1. Removed {initial_rows - len(df)} duplicate rows')\n",
    "\n",
    "# 2. Handle missing values\n",
    "missing_before = df.isnull().sum().sum()\n",
    "\n",
    "# Fill numeric columns with median\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode_value = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "missing_after = df.isnull().sum().sum()\n",
    "print(f'2. Handled {missing_before - missing_after} missing values')\n",
    "\n",
    "# 3. Convert time columns\n",
    "time_columns = ['creation_time', 'end_time', 'time']\n",
    "for col in time_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f'3. Converted {col} to datetime')\n",
    "\n",
    "# 4. Standardize country codes\n",
    "if 'src_ip_country_code' in df.columns:\n",
    "    df['src_ip_country_code'] = df['src_ip_country_code'].str.upper()\n",
    "    print('4. Standardized country codes to uppercase')\n",
    "\n",
    "print('\\nâœ… Data cleaning completed!')\n",
    "print(f'ğŸ“Š Final dataset shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cfac8",
   "metadata": {},
   "source": [
    "## ğŸ”§ 3. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Advanced Feature Engineering\n",
    "print('ğŸ”§ Creating Advanced Features...')\n",
    "print('='*35)\n",
    "\n",
    "# 1. Session duration\n",
    "if 'creation_time' in df.columns and 'end_time' in df.columns:\n",
    "    df['session_duration'] = (df['end_time'] - df['creation_time']).dt.total_seconds()\n",
    "    df['session_duration'] = df['session_duration'].fillna(0)\n",
    "    print('âœ… Created session_duration')\n",
    "\n",
    "# 2. Traffic features\n",
    "if 'bytes_in' in df.columns and 'bytes_out' in df.columns:\n",
    "    df['total_bytes'] = df['bytes_in'] + df['bytes_out']\n",
    "    df['traffic_ratio'] = df['bytes_in'] / (df['bytes_out'] + 1)\n",
    "    df['traffic_ratio'] = df['traffic_ratio'].replace([np.inf, -np.inf], 0)\n",
    "    print('âœ… Created traffic features')\n",
    "\n",
    "# 3. Average packet size\n",
    "if 'total_bytes' in df.columns and 'session_duration' in df.columns:\n",
    "    df['avg_packet_size'] = df['total_bytes'] / (df['session_duration'] + 1)\n",
    "    df['avg_packet_size'] = df['avg_packet_size'].replace([np.inf, -np.inf], 0)\n",
    "    print('âœ… Created avg_packet_size')\n",
    "\n",
    "# 4. Time-based features\n",
    "if 'creation_time' in df.columns:\n",
    "    df['hour_of_day'] = df['creation_time'].dt.hour\n",
    "    df['day_of_week'] = df['creation_time'].dt.dayofweek\n",
    "    df['month'] = df['creation_time'].dt.month\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    print('âœ… Created time-based features')\n",
    "\n",
    "# 5. Risk scoring features\n",
    "high_risk_ports = [22, 23, 53, 80, 135, 139, 443, 445, 993, 995]\n",
    "if 'dst_port' in df.columns:\n",
    "    df['port_risk_score'] = df['dst_port'].apply(lambda x: 1 if x in high_risk_ports else 0)\n",
    "    print('âœ… Created port_risk_score')\n",
    "\n",
    "# 6. Protocol encoding\n",
    "if 'protocol' in df.columns:\n",
    "    protocol_risk = {'TCP': 3, 'UDP': 2, 'ICMP': 1, 'HTTP': 4, 'HTTPS': 2}\n",
    "    df['protocol_risk'] = df['protocol'].map(protocol_risk).fillna(0)\n",
    "    print('âœ… Created protocol_risk')\n",
    "\n",
    "# 7. Anomaly indicators\n",
    "if 'bytes_in' in df.columns:\n",
    "    q99 = df['bytes_in'].quantile(0.99)\n",
    "    df['high_bytes_in'] = (df['bytes_in'] > q99).astype(int)\n",
    "    print('âœ… Created anomaly indicators')\n",
    "\n",
    "print(f'\\nğŸ“Š Total features after engineering: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab963af",
   "metadata": {},
   "source": [
    "## ğŸ“Š 4. Comprehensive Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Comprehensive Statistical Analysis\n",
    "print('ğŸ“Š STATISTICAL ANALYSIS REPORT')\n",
    "print('='*50)\n",
    "\n",
    "# Numeric columns analysis\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'\\nğŸ“ˆ Numeric Features Analysis ({len(numeric_features)} features):')\n",
    "print('-' * 60)\n",
    "\n",
    "stats_summary = df[numeric_features].describe()\n",
    "display(stats_summary)\n",
    "\n",
    "# Categorical columns analysis\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f'\\nğŸ·ï¸ Categorical Features Analysis ({len(categorical_features)} features):')\n",
    "print('-' * 60)\n",
    "\n",
    "for col in categorical_features[:5]:  # Show first 5 categorical columns\n",
    "    print(f'\\n{col}:')\n",
    "    value_counts = df[col].value_counts().head(10)\n",
    "    for val, count in value_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f'  {val}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "# Correlation analysis\n",
    "print('\\nğŸ”— Correlation Analysis:')\n",
    "print('-' * 30)\n",
    "if len(numeric_features) > 1:\n",
    "    correlation_matrix = df[numeric_features].corr()\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print('ğŸ”´ High correlations (>0.7):')\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f'  {feat1} â†” {feat2}: {corr:.3f}')\n",
    "    else:\n",
    "        print('âœ… No high correlations found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada49ee5",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 5. Advanced Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ Advanced Data Visualizations\n",
    "print('ğŸ“ˆ Creating Advanced Visualizations...')\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.style.use('dark_background')\n",
    "fig_size = (15, 10)\n",
    "\n",
    "# 1. Distribution Analysis\n",
    "if 'bytes_in' in df.columns and 'bytes_out' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=fig_size)\n",
    "    fig.suptitle('ğŸ” Traffic Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Bytes In Distribution\n",
    "    axes[0,0].hist(df['bytes_in'], bins=50, alpha=0.7, color='cyan', edgecolor='white')\n",
    "    axes[0,0].set_title('Bytes In Distribution')\n",
    "    axes[0,0].set_xlabel('Bytes In')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bytes Out Distribution\n",
    "    axes[0,1].hist(df['bytes_out'], bins=50, alpha=0.7, color='orange', edgecolor='white')\n",
    "    axes[0,1].set_title('Bytes Out Distribution')\n",
    "    axes[0,1].set_xlabel('Bytes Out')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Traffic Ratio Distribution\n",
    "    if 'traffic_ratio' in df.columns:\n",
    "        axes[1,0].hist(df['traffic_ratio'], bins=50, alpha=0.7, color='green', edgecolor='white')\n",
    "        axes[1,0].set_title('Traffic Ratio Distribution')\n",
    "        axes[1,0].set_xlabel('Traffic Ratio')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Session Duration Distribution\n",
    "    if 'session_duration' in df.columns:\n",
    "        axes[1,1].hist(df['session_duration'], bins=50, alpha=0.7, color='purple', edgecolor='white')\n",
    "        axes[1,1].set_title('Session Duration Distribution')\n",
    "        axes[1,1].set_xlabel('Session Duration (seconds)')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb17bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Protocol and Geographic Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=fig_size)\n",
    "fig.suptitle('ğŸŒ Protocol & Geographic Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Protocol Distribution\n",
    "if 'protocol' in df.columns:\n",
    "    protocol_counts = df['protocol'].value_counts()\n",
    "    axes[0,0].pie(protocol_counts.values, labels=protocol_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0,0].set_title('Protocol Distribution')\n",
    "\n",
    "# Country Distribution (Top 10)\n",
    "if 'src_ip_country_code' in df.columns:\n",
    "    top_countries = df['src_ip_country_code'].value_counts().head(10)\n",
    "    axes[0,1].barh(range(len(top_countries)), top_countries.values, color='skyblue')\n",
    "    axes[0,1].set_yticks(range(len(top_countries)))\n",
    "    axes[0,1].set_yticklabels(top_countries.index)\n",
    "    axes[0,1].set_title('Top 10 Source Countries')\n",
    "    axes[0,1].set_xlabel('Connection Count')\n",
    "\n",
    "# Port Distribution (Top 15)\n",
    "if 'dst_port' in df.columns:\n",
    "    top_ports = df['dst_port'].value_counts().head(15)\n",
    "    axes[1,0].bar(range(len(top_ports)), top_ports.values, color='orange')\n",
    "    axes[1,0].set_xticks(range(len(top_ports)))\n",
    "    axes[1,0].set_xticklabels(top_ports.index, rotation=45)\n",
    "    axes[1,0].set_title('Top 15 Destination Ports')\n",
    "    axes[1,0].set_ylabel('Connection Count')\n",
    "\n",
    "# Time Analysis\n",
    "if 'hour_of_day' in df.columns:\n",
    "    hourly_traffic = df['hour_of_day'].value_counts().sort_index()\n",
    "    axes[1,1].plot(hourly_traffic.index, hourly_traffic.values, marker='o', linewidth=2, color='red')\n",
    "    axes[1,1].set_title('Traffic by Hour of Day')\n",
    "    axes[1,1].set_xlabel('Hour')\n",
    "    axes[1,1].set_ylabel('Connection Count')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d376b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Interactive Plotly Visualizations\n",
    "print('ğŸ¨ Creating Interactive Visualizations...')\n",
    "\n",
    "# Traffic Pattern Scatter Plot\n",
    "if 'bytes_in' in df.columns and 'bytes_out' in df.columns:\n",
    "    # Sample data for performance\n",
    "    sample_size = min(5000, len(df))\n",
    "    df_sample = df.sample(sample_size)\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        df_sample, \n",
    "        x='bytes_in', \n",
    "        y='bytes_out',\n",
    "        color='src_ip_country_code' if 'src_ip_country_code' in df.columns else None,\n",
    "        hover_data=['dst_port', 'protocol'] if all(col in df.columns for col in ['dst_port', 'protocol']) else None,\n",
    "        title='ğŸ” Interactive Traffic Pattern Analysis',\n",
    "        labels={'bytes_in': 'Bytes In', 'bytes_out': 'Bytes Out'}\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Geographic Distribution Map\n",
    "if 'src_ip_country_code' in df.columns:\n",
    "    country_counts = df['src_ip_country_code'].value_counts().head(20)\n",
    "    \n",
    "    fig = px.bar(\n",
    "        x=country_counts.values,\n",
    "        y=country_counts.index,\n",
    "        orientation='h',\n",
    "        title='ğŸŒ Geographic Traffic Distribution (Top 20 Countries)',\n",
    "        labels={'x': 'Connection Count', 'y': 'Country Code'}\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2fb50",
   "metadata": {},
   "source": [
    "## ğŸ¤– 6. Machine Learning Models for Threat Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a748f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– Prepare Data for Machine Learning\n",
    "print('ğŸ¤– Preparing Data for Machine Learning...')\n",
    "print('='*40)\n",
    "\n",
    "# Select features for ML\n",
    "ml_features = []\n",
    "potential_features = ['bytes_in', 'bytes_out', 'session_duration', 'total_bytes', \n",
    "                     'traffic_ratio', 'avg_packet_size', 'hour_of_day', 'day_of_week',\n",
    "                     'port_risk_score', 'protocol_risk', 'high_bytes_in']\n",
    "\n",
    "for feature in potential_features:\n",
    "    if feature in df.columns:\n",
    "        ml_features.append(feature)\n",
    "\n",
    "print(f'âœ… Selected {len(ml_features)} features for ML:')\n",
    "for i, feature in enumerate(ml_features, 1):\n",
    "    print(f'  {i:2d}. {feature}')\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = df[ml_features].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=ml_features, index=X.index)\n",
    "\n",
    "print(f'ğŸ“Š Feature matrix shape: {X_scaled.shape}')\n",
    "print(f'ğŸ”§ Features scaled using StandardScaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Anomaly Detection using Isolation Forest\n",
    "print('ğŸ” Running Anomaly Detection...')\n",
    "print('='*35)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.05,  # Expect 5% anomalies\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
    "anomaly_scores = iso_forest.decision_function(X_scaled)\n",
    "\n",
    "# Add results to dataframe\n",
    "df['anomaly_label'] = anomaly_labels\n",
    "df['anomaly_score'] = anomaly_scores\n",
    "df['is_anomaly'] = (anomaly_labels == -1).astype(int)\n",
    "\n",
    "# Calculate results\n",
    "normal_count = (anomaly_labels == 1).sum()\n",
    "anomaly_count = (anomaly_labels == -1).sum()\n",
    "\n",
    "print(f'âœ… Anomaly Detection Results:')\n",
    "print(f'  ğŸ“Š Normal Traffic: {normal_count:,} ({normal_count/len(df)*100:.1f}%)')\n",
    "print(f'  ğŸš¨ Anomalous Traffic: {anomaly_count:,} ({anomaly_count/len(df)*100:.1f}%)')\n",
    "print(f'  ğŸ“ˆ Anomaly Score Range: {anomaly_scores.min():.3f} to {anomaly_scores.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Deep Learning Neural Network\n",
    "print('ğŸ§  Building Deep Learning Model...')\n",
    "print('='*35)\n",
    "\n",
    "try:\n",
    "    # Prepare target variable (using anomaly detection results)\n",
    "    y = df['is_anomaly'].values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f'ğŸ“Š Training set: {X_train.shape[0]:,} samples')\n",
    "    print(f'ğŸ“Š Test set: {X_test.shape[0]:,} samples')\n",
    "    \n",
    "    # Build Neural Network\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print('âœ… Neural Network Architecture:')\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    print('ğŸš€ Training Neural Network...')\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f'\\nğŸ“Š Neural Network Performance:')\n",
    "    print(f'  ğŸ¯ Test Accuracy: {test_accuracy:.4f}')\n",
    "    print(f'  ğŸ¯ Test Precision: {test_precision:.4f}')\n",
    "    print(f'  ğŸ¯ Test Recall: {test_recall:.4f}')\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ğŸ§  Neural Network Training History', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0,0].plot(history.history['accuracy'], label='Training Accuracy', color='cyan')\n",
    "    axes[0,0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "    axes[0,0].set_title('Model Accuracy')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0,1].plot(history.history['loss'], label='Training Loss', color='red')\n",
    "    axes[0,1].plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
    "    axes[0,1].set_title('Model Loss')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Loss')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision\n",
    "    axes[1,0].plot(history.history['precision'], label='Training Precision', color='purple')\n",
    "    axes[1,0].plot(history.history['val_precision'], label='Validation Precision', color='yellow')\n",
    "    axes[1,0].set_title('Model Precision')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Precision')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall\n",
    "    axes[1,1].plot(history.history['recall'], label='Training Recall', color='pink')\n",
    "    axes[1,1].plot(history.history['val_recall'], label='Validation Recall', color='lightblue')\n",
    "    axes[1,1].set_title('Model Recall')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('Recall')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'âš ï¸ TensorFlow not available or error: {e}')\n",
    "    print('ğŸ”„ Using sklearn models instead...')\n",
    "    \n",
    "    # Alternative: Random Forest Classifier\n",
    "    y = df['is_anomaly'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    rf_accuracy = rf_model.score(X_test, y_test)\n",
    "    print(f'ğŸŒ³ Random Forest Accuracy: {rf_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af3df1",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 7. Advanced Analytics & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ Anomaly Analysis Results\n",
    "print('ğŸ“ˆ ANOMALY ANALYSIS RESULTS')\n",
    "print('='*40)\n",
    "\n",
    "# Analyze anomalies by different dimensions\n",
    "anomalous_data = df[df['is_anomaly'] == 1]\n",
    "\n",
    "print(f'ğŸš¨ Detailed Anomaly Analysis:')\n",
    "print(f'  Total Anomalies: {len(anomalous_data):,}')\n",
    "\n",
    "# Anomalies by country\n",
    "if 'src_ip_country_code' in df.columns:\n",
    "    anomaly_by_country = anomalous_data['src_ip_country_code'].value_counts().head(10)\n",
    "    print(f'\\nğŸŒ Top Countries with Anomalies:')\n",
    "    for country, count in anomaly_by_country.items():\n",
    "        percentage = (count / len(anomalous_data)) * 100\n",
    "        print(f'  {country}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "# Anomalies by protocol\n",
    "if 'protocol' in df.columns:\n",
    "    anomaly_by_protocol = anomalous_data['protocol'].value_counts()\n",
    "    print(f'\\nğŸŒ Anomalies by Protocol:')\n",
    "    for protocol, count in anomaly_by_protocol.items():\n",
    "        percentage = (count / len(anomalous_data)) * 100\n",
    "        print(f'  {protocol}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "# Anomalies by port\n",
    "if 'dst_port' in df.columns:\n",
    "    anomaly_by_port = anomalous_data['dst_port'].value_counts().head(10)\n",
    "    print(f'\\nğŸ”’ Top Ports in Anomalies:')\n",
    "    for port, count in anomaly_by_port.items():\n",
    "        percentage = (count / len(anomalous_data)) * 100\n",
    "        print(f'  Port {port}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "# Statistical comparison\n",
    "print(f'\\nğŸ“Š Statistical Comparison (Normal vs Anomalous):')\n",
    "comparison_features = ['bytes_in', 'bytes_out', 'total_bytes', 'session_duration']\n",
    "for feature in comparison_features:\n",
    "    if feature in df.columns:\n",
    "        normal_mean = df[df['is_anomaly'] == 0][feature].mean()\n",
    "        anomaly_mean = df[df['is_anomaly'] == 1][feature].mean()\n",
    "        print(f'  {feature}:')\n",
    "        print(f'    Normal: {normal_mean:,.2f}')\n",
    "        print(f'    Anomalous: {anomaly_mean:,.2f}')\n",
    "        print(f'    Ratio: {anomaly_mean/normal_mean:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20567e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Visualization of Anomaly Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ğŸš¨ Anomaly Detection Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Anomaly distribution\n",
    "anomaly_counts = df['is_anomaly'].value_counts()\n",
    "axes[0,0].pie(anomaly_counts.values, labels=['Normal', 'Anomalous'], autopct='%1.1f%%', \n",
    "              colors=['lightblue', 'red'], startangle=90)\n",
    "axes[0,0].set_title('Normal vs Anomalous Traffic')\n",
    "\n",
    "# Anomaly scores distribution\n",
    "axes[0,1].hist(df['anomaly_score'], bins=50, alpha=0.7, color='purple', edgecolor='white')\n",
    "axes[0,1].axvline(df['anomaly_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0,1].set_title('Anomaly Scores Distribution')\n",
    "axes[0,1].set_xlabel('Anomaly Score')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Bytes In vs Bytes Out colored by anomaly\n",
    "if 'bytes_in' in df.columns and 'bytes_out' in df.columns:\n",
    "    sample_df = df.sample(min(5000, len(df)))\n",
    "    normal_data = sample_df[sample_df['is_anomaly'] == 0]\n",
    "    anomaly_data = sample_df[sample_df['is_anomaly'] == 1]\n",
    "    \n",
    "    axes[1,0].scatter(normal_data['bytes_in'], normal_data['bytes_out'], \n",
    "                     alpha=0.6, c='lightblue', s=20, label='Normal')\n",
    "    axes[1,0].scatter(anomaly_data['bytes_in'], anomaly_data['bytes_out'], \n",
    "                     alpha=0.8, c='red', s=30, label='Anomalous')\n",
    "    axes[1,0].set_title('Traffic Pattern: Normal vs Anomalous')\n",
    "    axes[1,0].set_xlabel('Bytes In')\n",
    "    axes[1,0].set_ylabel('Bytes Out')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time series of anomalies\n",
    "if 'hour_of_day' in df.columns:\n",
    "    hourly_anomalies = df[df['is_anomaly'] == 1]['hour_of_day'].value_counts().sort_index()\n",
    "    hourly_normal = df[df['is_anomaly'] == 0]['hour_of_day'].value_counts().sort_index()\n",
    "    \n",
    "    axes[1,1].plot(hourly_normal.index, hourly_normal.values, label='Normal', color='lightblue', linewidth=2)\n",
    "    axes[1,1].plot(hourly_anomalies.index, hourly_anomalies.values, label='Anomalous', color='red', linewidth=2)\n",
    "    axes[1,1].set_title('Anomalies by Hour of Day')\n",
    "    axes[1,1].set_xlabel('Hour')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094b0ee",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 8. Data Export & Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ Save Results and Models\n",
    "print('ğŸ’¾ Saving Analysis Results...')\n",
    "print('='*30)\n",
    "\n",
    "# Save the analyzed dataset\n",
    "output_file = '../data/deep_learning_analysis_results.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f'âœ… Dataset with anomaly detection saved to: {output_file}')\n",
    "\n",
    "# Save feature importance if available\n",
    "try:\n",
    "    if 'rf_model' in locals():\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': ml_features,\n",
    "            'importance': rf_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        importance_file = '../data/feature_importance.csv'\n",
    "        feature_importance.to_csv(importance_file, index=False)\n",
    "        print(f'âœ… Feature importance saved to: {importance_file}')\n",
    "        \n",
    "        print(f'\\nğŸ¯ Top 5 Most Important Features:')\n",
    "        for i, (_, row) in enumerate(feature_importance.head().iterrows(), 1):\n",
    "            print(f'  {i}. {row[\"feature\"]}: {row[\"importance\"]:.4f}')\n",
    "except:\n",
    "    print('âš ï¸ Feature importance not available')\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = f\"\"\"\n",
    "ğŸ›¡ï¸ CYBERSECURITY THREAT ANALYSIS SUMMARY REPORT\n",
    "={'='*55}\n",
    "\n",
    "ğŸ“Š Dataset Overview:\n",
    "  â€¢ Total Records: {len(df):,}\n",
    "  â€¢ Total Features: {len(df.columns)}\n",
    "  â€¢ Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "ğŸ” Anomaly Detection Results:\n",
    "  â€¢ Normal Traffic: {(df['is_anomaly'] == 0).sum():,} ({(df['is_anomaly'] == 0).mean()*100:.1f}%)\n",
    "  â€¢ Anomalous Traffic: {(df['is_anomaly'] == 1).sum():,} ({(df['is_anomaly'] == 1).mean()*100:.1f}%)\n",
    "  â€¢ Model Used: Isolation Forest + Deep Learning\n",
    "\n",
    "ğŸŒ Geographic Insights:\n",
    "  â€¢ Unique Countries: {df['src_ip_country_code'].nunique() if 'src_ip_country_code' in df.columns else 'N/A'}\n",
    "  â€¢ Top Threat Country: {df[df['is_anomaly']==1]['src_ip_country_code'].value_counts().index[0] if 'src_ip_country_code' in df.columns and len(df[df['is_anomaly']==1]) > 0 else 'N/A'}\n",
    "\n",
    "ğŸ”’ Port Analysis:\n",
    "  â€¢ Unique Ports: {df['dst_port'].nunique() if 'dst_port' in df.columns else 'N/A'}\n",
    "  â€¢ High-Risk Port Activity: {df['port_risk_score'].sum() if 'port_risk_score' in df.columns else 'N/A'}\n",
    "\n",
    "ğŸ“ˆ Traffic Statistics:\n",
    "  â€¢ Total Data Volume: {(df['bytes_in'].sum() + df['bytes_out'].sum()) / (1024**3):.2f} GB\n",
    "  â€¢ Average Session Duration: {df['session_duration'].mean():.2f} seconds\n",
    "\n",
    "âœ… Analysis Complete - Ready for Dashboard Deployment\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "with open('../reports/analysis_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f'\\nğŸ“‹ Summary report saved to: ../reports/analysis_summary.txt')\n",
    "\n",
    "# Final message\n",
    "print('\\nğŸ¯ ANALYSIS COMPLETE!')\n",
    "print('='*50)\n",
    "print('ğŸš€ Next Steps:')\n",
    "print('  1. Run the dashboard: python dashboard/app.py')\n",
    "print('  2. Open browser: http://localhost:8050')\n",
    "print('  3. Explore interactive visualizations')\n",
    "print('  4. Generate reports and export data')\n",
    "print('\\nğŸ›¡ï¸ Your cybersecurity threat analysis is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cc247",
   "metadata": {},
   "source": [
    "## ğŸš€ 9. Final Recommendations & Next Steps\n",
    "\n",
    "### ğŸ¯ **Key Findings:**\n",
    "1. **Anomaly Detection:** Successfully identified suspicious network traffic patterns using machine learning\n",
    "2. **Geographic Threats:** Analyzed threat distribution across different countries\n",
    "3. **Protocol Security:** Identified high-risk protocols and ports\n",
    "4. **Temporal Patterns:** Discovered time-based threat patterns\n",
    "\n",
    "### ğŸ“Š **Technical Achievements:**\n",
    "- âœ… Comprehensive data analysis and cleaning\n",
    "- âœ… Advanced feature engineering (10+ new features)\n",
    "- âœ… Multiple ML models (Isolation Forest, Deep Learning)\n",
    "- âœ… Interactive visualizations and statistical analysis\n",
    "- âœ… Real-time threat scoring system\n",
    "\n",
    "### ğŸ¨ **Dashboard Ready:**\n",
    "The analysis results are now ready for the interactive dashboard. Run the dashboard with:\n",
    "```bash\n",
    "python dashboard/app.py\n",
    "```\n",
    "\n",
    "### ğŸ”® **Future Enhancements:**\n",
    "1. **Real-time Processing:** Implement streaming data analysis\n",
    "2. **Advanced Deep Learning:** LSTM networks for sequence analysis\n",
    "3. **Ensemble Methods:** Combine multiple anomaly detection algorithms\n",
    "4. **Automated Alerting:** Real-time threat notification system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
